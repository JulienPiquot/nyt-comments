import NewYorkTimesComments.{sparkSession, _}
import org.apache.commons.math3.distribution.FDistribution
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.{ByteArrayDeserializer, StringDeserializer}
import org.apache.log4j.{Level, LogManager}
import org.apache.spark.SparkContext
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer}
import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}
import org.apache.spark.mllib.feature.Word2VecModel
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka010.{KafkaUtils, OffsetRange}
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
import org.apache.spark.sql.functions._


object KafkaIntegration {

  val sparkSession: SparkSession = SparkSession.builder.
    master("local[*]")
    .appName("NYT Comments")
    //.config("spark.driver.cores", "4")
    //.config("spark.executor.cores", "4")
    .getOrCreate()
  LogManager.getRootLogger.setLevel(Level.WARN)
  //val ssc = new StreamingContext(sparkSession.sparkContext, Seconds(10))

  import scala.collection.JavaConverters._
  import sparkSession.implicits._

  val kafkaParams: Map[String, Object] = Map[String, Object](
    "bootstrap.servers" -> "localhost:9092",
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[ByteArrayDeserializer],
    "group.id" -> "test_group",
    //"auto.offset.reset" -> "latest",
    "enable.auto.commit" -> (false: java.lang.Boolean),
    "auto.offset.reset" -> "earliest"
  )

  def main(args: Array[String]): Unit = {
    val comments: RDD[Comment] = buildCommentsRdd(sparkSession.sparkContext).sample(withReplacement = false, fraction = 0.0001d).repartition(24)

    // calcul de l'ANOVA - rejet de l'hypothÃ¨se nulle potentiellement due a un effet "Big Data"
    //computeAnova(comments)

    // calcul des tfidfs
    var commentDF = sparkSession.createDataFrame(comments.map(c => Row.fromSeq(Seq(c.getCommentID, c.getCommentBody, c.getCommentType, c.getCreateDate, c.getDepth, c.isEditorsSelection, c.getRecommandations, c.getReplyCount, c.getSharing, c.getUserDisplayName, c.getUseLocation, c.getArticleID, c.getNewDesk, c.getArticleWordCount, c.getPrintPage, c.getTypeOfMaterial))),
      NewYorkTimesComments.commentSchema).cache()
    println("df size is " + commentDF.count() + " comments")
    val preprocessTextUdf = udf((t: String) => {
      val sentiments = sentimentAnalysis(t)
      (sentiments.tokens, sentiments.sentiments)
    })
    val selectTokens = udf((nlpResult: Row) => nlpResult.getAs[Seq[String]](0))
    val selectSentiments = udf((nlpResult: Row) => nlpResult.getAs[Vector](1))
    commentDF = commentDF.withColumn("nlp", preprocessTextUdf($"commentBody")).cache()
    commentDF = commentDF.withColumn("tokens", selectTokens($"nlp")).withColumn("sentimentVector", selectSentiments($"nlp"))

    val (tfidfDF, bVocabulary) = compteAndCacheTfIdf(commentDF)
    commentDF = tfidfDF.cache()
    commentDF.limit(10).foreach(print(_))
    println("number of partitions commentDF : " + commentDF.rdd.getNumPartitions)
    println("number of comments : " + commentDF.count())

    // entrainement du modele Word2Vec
    //val w2vModel = computeW2V(commentDF)
    val w2vModel = Word2VecModel.load(sparkSession.sparkContext, "comments-w2v")

    // vectorisation du texte grace aux representations w2v
    var vectorised = vectorizeText(commentDF, bVocabulary.value, w2vModel)

    // calcul de la decroissance des inerties intra classes
    //computeKMeansInerties(vectorised)

    // choix d'une valeur k et visualisation des classes
    //computeAndVisualizeKMeans(w2vModel, vectorised, 10)

    // run LSA in order to compare with
    //lsa(commentDF, "commentBody", "tokens", preprocess = false)

    // run the decision tree regression
    vectorised = normalisePredictions(vectorised)

    runDecisionTree(vectorised)
  }

  def runSentimentAnalysis(df: DataFrame): DataFrame = {
    val sentAnalysis = udf((text: String) => NewYorkTimesComments.sentimentAnalysis(text))
    df.withColumn("sentimentVector", sentAnalysis($"commentBody")).cache()
  }

  def normalisePredictions(df: DataFrame): DataFrame = {
    val normReco = udf((n: Int, d: Int) => n.toDouble / d.toDouble)
    val logReco = udf((r: Int) => if(r == 0) -1 else Math.log10(r))
    val sumDf = df.groupBy($"articleID").sum("recommandations").filter(r => r.getAs[Long]("sum(recommandations)") > 0).cache()
    val normDf = df.join(sumDf, "articleID")
      .withColumn("normRecommandations", normReco($"recommandations", $"sum(recommandations)"))
      .withColumn("logRecommandations", logReco($"recommandations")).cache()
    normDf.select("recommandations", "normRecommandations", "logRecommandations").summary().show()
    normDf
    //printHist("src/main/gnuplot/logrecommandation_hist.txt", normDf.map(r => r.getAs[Double]("logRecommandations")).rdd.histogram(20))
  }

  def runDecisionTree(comments: DataFrame): Unit = {
//    val toMLVector = udf((v: Vector, wc: Int, printPage: Int, typeOfMaterial: String, newDesk: String) => new org.apache.spark.ml.linalg.DenseVector(v.toArray ++ Array(wc.toDouble, printPage.toDouble, typeOfMaterial.hashCode.toDouble, newDesk.hashCode.toDouble)))
    val toMLVector = udf((v: Vector, s: Vector, wc: Int) => new org.apache.spark.ml.linalg.DenseVector(v.toArray ++ s.toArray ++ Array(wc.toDouble)))

    val data: DataFrame = comments.withColumn("mlvector", toMLVector($"vector", $"sentimentVector", $"articleWordCount")).cache()
    data.show(10)

    val varToPredict = "normRecommandations"

    val featureIndexer = new VectorIndexer()
      .setInputCol("mlvector")
      .setOutputCol("indexedVector")
      .setMaxCategories(60)
      .fit(data)
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))
    val dt = new DecisionTreeRegressor()
      .setMaxBins(64)
      .setMinInstancesPerNode(2)
      .setLabelCol(varToPredict)
      .setFeaturesCol("indexedVector")
    val pipeline = new Pipeline().setStages(Array(featureIndexer, dt))
    val model = pipeline.fit(trainingData)
    val predictions = model.transform(testData)
    predictions.select("prediction", varToPredict, "indexedVector").show(10)
    val evaluator = new RegressionEvaluator()
      .setLabelCol(varToPredict)
      .setPredictionCol("prediction")
      .setMetricName("rmse")
    val rmse = evaluator.evaluate(predictions)
    println("Root Mean Squared Error (RMSE) on test data = " + rmse)
    val treeModel = model.stages(1).asInstanceOf[DecisionTreeRegressionModel]
    println("Learned regression tree model:\n" + treeModel.toDebugString)
  }

  def vectorizeText(comments: DataFrame, voc: Array[String], w2v: Word2VecModel): DataFrame = {
    val embeddings = sparkSession.sparkContext.broadcast(w2v.getVectors)
    val vectorised = addVector(embeddings.value, voc, comments).cache()
    println("number of vectorized paritions : " + vectorised.rdd.getNumPartitions)
    println("### embeddings examples ###")
    vectorised.head(10).foreach(row => {
      println(row.getAs[String]("commentBody"))
      w2v.findSynonyms(row.getAs[Vector]("vector"), 10).foreach(println(_))
    })
    vectorised
  }

  def computeW2V(comments: DataFrame): Word2VecModel = {
    val corpus = buildW2VCorpus(comments).cache()
    println("number of paragraphs : " + corpus.count())
    println("number of partitions corpus : " + corpus.getNumPartitions)
    val w2vModel = buildW2VModel(corpus)
    w2vModel.save(sparkSession.sparkContext, "comments-w2v")
    w2vModel
  }

  def computeKMeansInerties(comments: DataFrame): Unit = {
    println("number of vectorized paritions : " + comments.rdd.getNumPartitions)
    println("### embeddings ###")
    for (i <- 1 to 20) {
      println("### kmeans k = " + i + " ###")
      kmeans(comments, i, null)
    }
  }

  def computeAndVisualizeKMeans(w2v: Word2VecModel, comments: DataFrame, k: Int): Unit = {
    kmeans(comments, k, w2v)
  }

  def computeAnova(comments: RDD[Comment]): Unit = {
    val stats = Anova.getAnovaStats(sparkSession, comments.map(comment => CatTuple(comment.getArticleID, comment.getRecommandations)).toDS())
    println(stats)
    val fdist: FDistribution = new FDistribution(null, stats.dfb, stats.dfw)
    println("p value is :" + (1.0 - fdist.cumulativeProbability(stats.F_value)))
  }

  def buildCommentsRdd(sparkContext: SparkContext): RDD[Comment] = {
    val offsetRanges = Array(
      // topic, partition, inclusive starting offset, exclusive ending offset
      // max num is 544085
      OffsetRange("comments", 0, 0, 544085),
      OffsetRange("comments", 1, 0, 544085),
      OffsetRange("comments", 2, 0, 544085),
      OffsetRange("comments", 3, 0, 544085)
    )

    KafkaUtils.createRDD[String, Array[Byte]](sparkContext, kafkaParams.asJava, offsetRanges, PreferConsistent)
      .map(record => Comment.deserialize(record.value))
  }

  def streamArticles(ssc: StreamingContext): Unit = {
    val topics = Array("test")
    val stream: DStream[(String, String)] = KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](topics, kafkaParams)
    ).map(record => (record.key, record.value))

    println("Start streaming ...")
    stream.foreachRDD(rdd => {
      println("rdd size " + rdd.count())
      rdd.foreach(kv => println(kv._2))
    })
    ssc.start()
    ssc.awaitTermination()
    //ssc.stop(stopSparkContext = true, stopGracefully = true)
  }
}
